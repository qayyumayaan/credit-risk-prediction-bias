{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99abf03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/application_train.csv')\n",
    "test  = pd.read_csv('../data/application_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)\n",
    "# df.sample(20)\n",
    "# df.info()\n",
    "# df.columns\n",
    "\n",
    "# print(df.NAME_TYPE_SUITE.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "print(train['DAYS_EMPLOYED'].describe())\n",
    "\n",
    "print((train['DAYS_BIRTH'] / -365).describe())\n",
    "\n",
    "# Create an anomalous flag column\n",
    "train['DAYS_EMPLOYED_ANOM'] = train[\"DAYS_EMPLOYED\"] == 365243\n",
    "\n",
    "# Replace the anomalous values with nan\n",
    "train['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "\n",
    "train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Days Employment');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "\n",
    "# Missing values statistics\n",
    "missing_values = missing_values_table(train)\n",
    "missing_values.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8025857",
   "metadata": {},
   "source": [
    "There are a lot with missing values. The models I am training can handle missing values natively, but many make a boolean missing indicator feature like so: \n",
    "```python\n",
    "df['COMMONAREA_MEDI_missing'] = df['COMMONAREA_MEDI'].isnull().astype(int)\n",
    "df['COMMONAREA_MEDI'] = df['COMMONAREA_MEDI'].fillna(-1)\n",
    "\n",
    "# Imputation is also a technique that can be used. Like so: \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Make sure to drop the ids and target\n",
    "train = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "test = test.drop(columns = ['SK_ID_CURR'])\n",
    "```\n",
    "\n",
    "There are many forms of imputation but this is one form with PCA: \n",
    "```python\n",
    "# Make a pipeline with imputation and pca\n",
    "pipeline = Pipeline(steps = [('imputer', Imputer(strategy = 'median')),\n",
    "             ('pca', PCA())])\n",
    "\n",
    "# Fit and transform on the training data\n",
    "train_pca = pipeline.fit_transform(train)\n",
    "\n",
    "# transform the testing data\n",
    "test_pca = pipeline.transform(test)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e86111",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    \"SK_ID_CURR\", \"OWN_CAR_AGE\", \"DAYS_EMPLOYED\",\n",
    "    \"WEEKDAY_APPR_PROCESS_START\", \"HOUR_APPR_PROCESS_START\",\n",
    "    \"WALLSMATERIAL_MODE\"\n",
    "]\n",
    "train.drop(columns=cols_to_drop, errors=\"ignore\", inplace=True)\n",
    "test.drop(columns=cols_to_drop, errors=\"ignore\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ae81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Split features/label\n",
    "y = train[\"TARGET\"]\n",
    "X = train.drop(columns=[\"TARGET\"])\n",
    "\n",
    "train_X, test_X = train_X.align(test, join='inner', axis=1)\n",
    "\n",
    "print(\"Training shape:\", train_X.shape)\n",
    "print(\"Testing shape:\", test_X.shape)\n",
    "\n",
    "# Categorical columns\n",
    "one_hot_cols = [\n",
    "    'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
    "    'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',\n",
    "    'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE',\n",
    "    'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE',\n",
    "    'EMERGENCYSTATE_MODE'\n",
    "]\n",
    "\n",
    "# Split into binary vs multi-class categorical\n",
    "binary_cats = []\n",
    "multi_cats = []\n",
    "\n",
    "# Keep only columns that actually exist after align\n",
    "one_hot_cols = [c for c in one_hot_cols if c in train_X.columns]\n",
    "\n",
    "\n",
    "for col in one_hot_cols:\n",
    "    # Count unique *string* categories (drop NaN)\n",
    "    unique_vals = X[col].dropna().unique()\n",
    "    if len(unique_vals) <= 2:\n",
    "        binary_cats.append(col)\n",
    "    else:\n",
    "        multi_cats.append(col)\n",
    "\n",
    "print(\"Binary categorical columns:\", binary_cats)\n",
    "print(\"Multi-class categorical columns:\", multi_cats)\n",
    "\n",
    "# Build the column transformer\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"binary\", OrdinalEncoder(), binary_cats),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown='ignore', sparse_output=False), multi_cats)\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # keep numerical columns\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_enc = ct.fit_transform(train_X)\n",
    "X_test_enc = ct.transform(test_X)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = ct.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "def scores(y_true, y_pred, y_pred_proba):\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "    print(\"F1:\", f1_score(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_true, y_pred_proba))\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    solver=\"liblinear\",\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=200\n",
    ")\n",
    "\n",
    "# CV predictions\n",
    "y_pred = cross_val_predict(logreg, X_enc, y, cv=5, method=\"predict\")\n",
    "y_pred_proba = cross_val_predict(logreg, X_enc, y, cv=5, method=\"predict_proba\")[:, 1]\n",
    "\n",
    "print(\"\\nLogistic Regression CV Scores\")\n",
    "scores(y, y_pred, y_pred_proba)\n",
    "\n",
    "# Fit final model\n",
    "logreg.fit(X_enc, y)\n",
    "\n",
    "# Predict on test set\n",
    "logreg_test_pred = logreg.predict_proba(X_test_enc)[:, 1]\n",
    "\n",
    "# Feature importance\n",
    "coef_importance = pd.Series(\n",
    "    abs(logreg.coef_[0]),\n",
    "    index=feature_names\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Logistic Regression Features:\")\n",
    "print(coef_importance.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d597165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# LightGBM Model\n",
    "lgbm = LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    metric=\"auc\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.02,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Cross-validated predictions (5-fold)\n",
    "lgb_pred = cross_val_predict(\n",
    "    lgbm, X_enc, y,\n",
    "    cv=5,\n",
    "    method=\"predict\"\n",
    ")\n",
    "\n",
    "lgb_pred_proba = cross_val_predict(\n",
    "    lgbm, X_enc, y,\n",
    "    cv=5,\n",
    "    method=\"predict_proba\"\n",
    ")[:, 1]\n",
    "\n",
    "print(\"\\nLightGBM CV Scores\")\n",
    "scores(y, lgb_pred, lgb_pred_proba)\n",
    "\n",
    "# Fit final model on ALL training data\n",
    "lgbm.fit(X_enc, y)\n",
    "\n",
    "# # Predict on real test data\n",
    "# lgbm_test_pred = lgbm.predict_proba(X_test_enc)[:, 1]\n",
    "\n",
    "# Feature importance\n",
    "lgb_importance = pd.Series(\n",
    "    lgbm.feature_importances_,\n",
    "    index=feature_names\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 LightGBM Features:\")\n",
    "print(lgb_importance.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
