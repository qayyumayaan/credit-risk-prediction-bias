{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99abf03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/application_train.csv')\n",
    "test  = pd.read_csv('../data/application_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)\n",
    "# df.sample(20)\n",
    "# df.info()\n",
    "# df.columns\n",
    "\n",
    "# print(df.NAME_TYPE_SUITE.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e86111",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    \"SK_ID_CURR\", \"OWN_CAR_AGE\", \"DAYS_EMPLOYED\",\n",
    "    \"WEEKDAY_APPR_PROCESS_START\", \"HOUR_APPR_PROCESS_START\",\n",
    "    \"WALLSMATERIAL_MODE\"\n",
    "]\n",
    "train.drop(columns=cols_to_drop, errors=\"ignore\", inplace=True)\n",
    "test.drop(columns=cols_to_drop, errors=\"ignore\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c95e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a lot of useless features, so L1 regularization is very useful here. \n",
    "# I will initially train a Logistic Regression model with sklearn. \n",
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ae81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Split features/label\n",
    "y = train[\"TARGET\"]\n",
    "X = train.drop(columns=[\"TARGET\"])\n",
    "\n",
    "# Your original categorical columns\n",
    "one_hot_cols = [\n",
    "    'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
    "    'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',\n",
    "    'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE',\n",
    "    'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE',\n",
    "    'EMERGENCYSTATE_MODE'\n",
    "]\n",
    "\n",
    "# Split into binary vs multi-class categorical\n",
    "binary_cats = []\n",
    "multi_cats = []\n",
    "\n",
    "for col in one_hot_cols:\n",
    "    # Count unique *string* categories (drop NaN)\n",
    "    unique_vals = X[col].dropna().unique()\n",
    "    if len(unique_vals) <= 2:\n",
    "        binary_cats.append(col)\n",
    "    else:\n",
    "        multi_cats.append(col)\n",
    "\n",
    "print(\"Binary categorical columns:\", binary_cats)\n",
    "print(\"Multi-class categorical columns:\", multi_cats)\n",
    "\n",
    "# Build the column transformer\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"binary\", OrdinalEncoder(), binary_cats),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown='ignore', sparse_output=False), multi_cats)\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # keep numerical columns\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_enc = ct.fit_transform(X)\n",
    "X_test_enc = ct.transform(test)\n",
    "\n",
    "# Get output feature names\n",
    "feature_names = ct.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Score function \n",
    "def scores(y_true, y_pred, y_pred_proba):\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "    print(\"F1:\", f1_score(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_true, y_pred_proba))\n",
    "\n",
    "# Logistic Regression Model\n",
    "logreg = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    solver=\"liblinear\",\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=200\n",
    ")\n",
    "\n",
    "# Cross-validated predictions (5-fold) on training set\n",
    "y_pred = cross_val_predict(\n",
    "    logreg, X_enc, y,\n",
    "    cv=5,\n",
    "    method=\"predict\"\n",
    ")\n",
    "\n",
    "y_pred_proba = cross_val_predict(\n",
    "    logreg, X_enc, y,\n",
    "    cv=5,\n",
    "    method=\"predict_proba\"\n",
    ")[:, 1]\n",
    "\n",
    "print(\"\\nLogistic Regression CV Scores\")\n",
    "scores(y, y_pred, y_pred_proba)\n",
    "\n",
    "# Fit final model on all training data\n",
    "logreg.fit(X_enc, y)\n",
    "\n",
    "# Predict on real test data\n",
    "logreg_test_pred = logreg.predict_proba(X_test_enc)[:, 1]\n",
    "\n",
    "# Optional: Logistic Regression feature importance\n",
    "coef_importance = pd.Series(\n",
    "    abs(logreg.coef_[0]),\n",
    "    index=feature_names\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Logistic Regression Features:\")\n",
    "print(coef_importance.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d597165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# LightGBM Model\n",
    "lgbm = LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    metric=\"auc\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.02,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Cross-validated predictions (5-fold)\n",
    "lgb_pred = cross_val_predict(\n",
    "    lgbm, X_enc, y,\n",
    "    cv=5,\n",
    "    method=\"predict\"\n",
    ")\n",
    "\n",
    "lgb_pred_proba = cross_val_predict(\n",
    "    lgbm, X_enc, y,\n",
    "    cv=5,\n",
    "    method=\"predict_proba\"\n",
    ")[:, 1]\n",
    "\n",
    "print(\"\\nLightGBM CV Scores\")\n",
    "scores(y, lgb_pred, lgb_pred_proba)\n",
    "\n",
    "# Fit final model on ALL training data\n",
    "lgbm.fit(X_enc, y)\n",
    "\n",
    "# Predict on real test data\n",
    "lgbm_test_pred = lgbm.predict_proba(X_test_enc)[:, 1]\n",
    "\n",
    "# Feature importance\n",
    "lgb_importance = pd.Series(\n",
    "    lgbm.feature_importances_,\n",
    "    index=feature_names\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 LightGBM Features:\")\n",
    "print(lgb_importance.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
